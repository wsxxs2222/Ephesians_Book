{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "import util\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from zhpr.predict import DocumentDataset, merge_stride, decode_pred\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def restore_punctuation(text: str,\n",
    "                        model_name: str = \"p208p2002/zh-wiki-punctuation-restore\",\n",
    "                        window_size: int = 256,\n",
    "                        step: int = 200,\n",
    "                        batch_size: int = 4,\n",
    "                        device: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Restore Chinese punctuation in `text` using the zh-wiki-punctuation-restore model,\n",
    "    without corrupting original characters.\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dataset = DocumentDataset(text, window_size=window_size, step=step)\n",
    "    dataloader = DataLoader(dataset=dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    def predict_batch(batch_input_ids):\n",
    "        outputs = model(input_ids=batch_input_ids.to(device))\n",
    "        logits = outputs.logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1).detach().cpu()\n",
    "        return pred_ids\n",
    "\n",
    "    for batch in dataloader:\n",
    "        if isinstance(batch, dict) and \"input_ids\" in batch:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "        else:\n",
    "            input_ids = batch\n",
    "        pred_ids = predict_batch(input_ids)\n",
    "\n",
    "        batch_out = []\n",
    "        for batch_pred_ids, batch_input in zip(pred_ids, input_ids):\n",
    "            tokens = tokenizer.convert_ids_to_tokens(batch_input)\n",
    "            input_ids_list = batch_input.tolist()\n",
    "            try:\n",
    "                pad_index = input_ids_list.index(tokenizer.pad_token_id)\n",
    "            except ValueError:\n",
    "                pad_index = len(input_ids_list)\n",
    "            tokens = tokens[:pad_index]\n",
    "            preds = batch_pred_ids[:pad_index]\n",
    "            pred_labels = [model.config.id2label[p.item()] for p in preds]\n",
    "            out = list(zip(tokens, pred_labels))\n",
    "            batch_out.append(out)\n",
    "        all_predictions.extend(batch_out)\n",
    "\n",
    "    merged = merge_stride(all_predictions, step)\n",
    "    decoded = decode_pred(merged)\n",
    "\n",
    "    # Fix: replace [UNK] with original characters\n",
    "    result_chars = []\n",
    "    orig_index = 0\n",
    "    for token in decoded:\n",
    "        if token == \"[UNK]\" and orig_index < len(text):\n",
    "            result_chars.append(text[orig_index])\n",
    "        else:\n",
    "            result_chars.append(token)\n",
    "        if token not in [\"，\", \"。\", \"？\", \"！\", \"；\", \"、\"]:  # only advance for non-inserted punctuation\n",
    "            orig_index += 1\n",
    "\n",
    "    result = ''.join(result_chars)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_list_to_string(object_list):\n",
    "    merged_string = \"\"\n",
    "    for obj in object_list:\n",
    "        merged_string += obj[\"content\"]\n",
    "    return merged_string\n",
    "\n",
    "def normalize_punctuation(text: str) -> str:\n",
    "    # map half-width punctuation to full-width\n",
    "    mapping = {\n",
    "        \",\": \"，\",\n",
    "        \";\": \"；\",\n",
    "        \"!\": \"！\",\n",
    "        \"?\": \"？\",\n",
    "        \".\": \"。\"\n",
    "    }\n",
    "    for en, zh in mapping.items():\n",
    "        text = text.replace(en, zh)\n",
    "    return text\n",
    "\n",
    "def strip_punctuation(text: str) -> str:\n",
    "    # Chinese + Western punctuations the model predicts\n",
    "    return re.sub(r\"[，。、！？；]\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_object_list_to_punctuated_string(content_object_list):\n",
    "    merged_string = object_list_to_string(content_object_list)\n",
    "    merged_string = normalize_punctuation(merged_string)\n",
    "    merged_string = strip_punctuation(merged_string)\n",
    "    punctuated_string = restore_punctuation(merged_string)\n",
    "    return punctuated_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuate_content_file(start_index, end_index):\n",
    "    for i in range(start_index, end_index):\n",
    "        content_file = util.file_name_builder(\".\" + util.FOLDER_PATH + \"text_from_audio/\",\n",
    "                                           \"Eph\", \"json\", i)\n",
    "        content_string_file = util.file_name_builder(\".\" + util.FOLDER_PATH + \"punctuated_content/\",\n",
    "                                           \"Eph\", \"txt\", i)\n",
    "        with open(content_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content_object_list = json.load(f)\n",
    "        punctuated_string = content_object_list_to_punctuated_string(content_object_list)\n",
    "            \n",
    "        with open(content_string_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(punctuated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuate_content_file(4, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latex part\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_latex_special_characters(c):\n",
    "  match c:\n",
    "    case \"&\" | \"%\" | \"$\" | \"#\" | \"_\" | \"{\" | \"}\":\n",
    "      return \"\\\\\" + c\n",
    "    case \"~\":\n",
    "      return \"\\\\textasciitilde\"\n",
    "    case \"^\":\n",
    "      return \"\\\\textasciicircum\"\n",
    "    case \"\\\\\":\n",
    "      return \"\\\\textbackslash\"\n",
    "    case _:\n",
    "      return c\n",
    "\n",
    "def modify_string_for_latex(s):\n",
    "  new_string = \"\"\n",
    "  for c in s:\n",
    "    new_string += handle_latex_special_characters(c)\n",
    "  return new_string\n",
    "\n",
    "def string_title_to_latex(title_object_list, content_string, output_file):\n",
    "    level_to_cmd = {\n",
    "        0: \"\\\\section\",\n",
    "        1: \"\\\\subsection\",\n",
    "        2: \"\\\\subsubsection\",\n",
    "    }\n",
    "\n",
    "    latex_lines = [\n",
    "        r\"\\documentclass[lang=cn,newtx,10pt,scheme=chinese]{elegantbook}\",  # ctex for Chinese\n",
    "        r\"\\begin{document}\",\n",
    "        r\"\\usepackage{graphicx}\",\n",
    "        r\"\\usepackage{fontspec}\",\n",
    "        r\"\\setmainfont{Times New Roman}[\",\n",
    "        \"  Ligatures=TeX,\"\n",
    "        \"  Script=Latin,\",\n",
    "        \"  Script=Greek\",\n",
    "        r\"]\",\n",
    "        \"\",\n",
    "        r\"\\tcbset{\",\n",
    "        r\"  mybox/.style={\",\n",
    "        \"    colframe=black,\",\n",
    "        \"    colback=white,\",\n",
    "        \"    boxrule=0.8pt,\",\n",
    "        \"    arc=0mm,\",\n",
    "        \"    left=6pt,\",\n",
    "        \"    right=6pt,\",\n",
    "        \"    top=6pt,\",\n",
    "        \"    bottom=6pt\",\n",
    "        r\"  }\",\n",
    "        r\"}\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    # list all titles\n",
    "    for title_object in title_object_list:\n",
    "        title_text = modify_string_for_latex(title_object[\"content\"].strip())\n",
    "        latex_command = level_to_cmd.get(title_object[\"level\"], \"\\\\subsubsection\")\n",
    "        latex_lines.append(latex_command + \"{\" + title_text + \"}\" + \"\\n\")\n",
    "    latex_lines.append(\"\\n\")\n",
    "    # cut the content into lines\n",
    "    latex_lines = format_content_string(content_string, latex_lines)\n",
    "    \n",
    "    latex_lines.append(r\"\\end{document}\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\".join(latex_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_segment_list(text):\n",
    "    parts = re.findall(r'.*?[，、。？！；]|[^，、。？！；]+', text)\n",
    "    parts = [p.strip() for p in parts if p.strip()]\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 sentence or k characters\n",
    "def format_content_string(content_string, latex_lines, line_limit=40):\n",
    "    print(f\"start length of latex_lines is {len(latex_lines)}\")\n",
    "    segment_list = string_to_segment_list(content_string)\n",
    "    current_line_word_count = 0\n",
    "    current_line = \"\"\n",
    "    for segment in segment_list:\n",
    "        punctuation = segment[-1]\n",
    "        # segment contains colon\n",
    "        if punctuation == \"。\":\n",
    "            current_line += segment\n",
    "            latex_lines.append(current_line + \"\\n\")\n",
    "            current_line = \"\"\n",
    "            current_line_word_count = 0\n",
    "        # segment exceeds character limit\n",
    "        elif current_line_word_count + len(segment) > line_limit:\n",
    "            current_line += segment\n",
    "            latex_lines.append(current_line + \"\\n\")\n",
    "            current_line = \"\"\n",
    "            current_line_word_count = 0\n",
    "        # normal case\n",
    "        else:\n",
    "            current_line += segment\n",
    "            current_line_word_count += len(segment)\n",
    "    print(f\"end length of latex_lines is {len(latex_lines)}\")\n",
    "    return latex_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_file_to_latex(start_index, end_index):\n",
    "    for i in range(start_index, end_index):\n",
    "        title_file = util.file_name_builder(\".\" + util.FOLDER_PATH + \"title_from_ppt/\", \"Eph\", \"json\", i)\n",
    "        content_file = util.file_name_builder(\".\" + util.FOLDER_PATH + \"punctuated_content/\", \"Eph\", \"txt\", i)\n",
    "        output_file = util.file_name_builder(\".\" + util.FOLDER_PATH + \"latex/\", \"Eph\", \"tex\", i)\n",
    "        with open(title_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            title_object_list = json.load(f)\n",
    "        with open(content_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content_string = f.read()\n",
    "        string_title_to_latex(title_object_list, content_string, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_common_misspell(content_string):\n",
    "    replacement_json_file = util.file_name_builder(\".\" + util.FOLDER_PATH + \"project_data/\", \"replace_misspelled_words\", \"json\")\n",
    "    with open(replacement_json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            replacement_dict = json.load(f)\n",
    "    for old_word, new_word in replacement_dict.items():\n",
    "        content_string = content_string.replace(old_word, new_word)\n",
    "    return content_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_file_to_latex(4, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
